{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvEH1+K4Bd0XyOiYnSH1OZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielDialektico/rag_agentes_langchain_curso/blob/main/notebooks/langchain_splitting_documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://dialektico.com/wp-content/uploads/2023/03/MiniLogoW4.png\" alt=\"Dial칠ktico Logo\" />"
      ],
      "metadata": {
        "id": "Vc-8Grt7ORmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este peque침o tutorial pertenece al curso de RAG y agentes con LangChain al que puedes acceder mediante la siguiente URL: https://www.youtube.com/playlist?list=PLlWTv9_GeWd32stuEMWpYOnxiVxnXaU6q\n",
        "\n",
        "Sigue los videos del curso para recibir instrucciones y contexto sobre la ejecuci칩n de este Notebook."
      ],
      "metadata": {
        "id": "FdVKmZZtOT3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "ad-poWzqOWWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Se instalan e importan las librer칤as"
      ],
      "metadata": {
        "id": "qFIVZvp0OZ2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.20\n",
        "!pip install langchain-community==0.3.20\n",
        "!pip install pypdf==5.4.0\n",
        "!pip install beautifulsoup4==4.13.3\n",
        "!pip install tiktoken==0.9.0"
      ],
      "metadata": {
        "id": "8U_GtE_0tuGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dTpru6JOFTj"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "import bs4\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "-FoCFQISWQSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar y dividir documentos con LangChain"
      ],
      "metadata": {
        "id": "7HnRYF3bHbYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un paso fundamental en un sistema RAG es la carga y divisi칩n de documentos en fragmentos informativos, los cuales ser치n posteriormente vectorizados y almacenados en una base de datos.\n",
        "\n",
        "Seguiremos los siguientes pasos:\n",
        "\n",
        "**Cargar**: Cargar documentos para su divisi칩n. Esto se hace con Document Loaders (cargadores de documentos).\n",
        "\n",
        "**Dividir**: Utizaremos Text Splitters (divisores de texto), los cuales dividen documentos grandes en fragmentos m치s peque침os. Esto es 칰til tanto para indexar los datos como para pasarlos a un modelo, ya que los fragmentos grandes son m치s dif칤ciles de buscar y no caben en la ventana de contexto limitada de un modelo."
      ],
      "metadata": {
        "id": "PIbYz1WkP3Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de informaci칩n de PDF"
      ],
      "metadata": {
        "id": "TcvsoVYZCPCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se carga el documento PDF.\n",
        "loader = PyPDFLoader('/content/Aviso_de_privacidad.pdf')\n",
        "pdf_pages = []\n",
        "async for page in loader.alazy_load():\n",
        "    pdf_pages.append(page)"
      ],
      "metadata": {
        "id": "XHHwywzLv15h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprime la lista.\n",
        "pdf_pages"
      ],
      "metadata": {
        "id": "Th-pk2vyv7rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprimen los metadatos de una p치gina.\n",
        "print(f\"{pdf_pages[0].metadata}\\n\")"
      ],
      "metadata": {
        "id": "IPUFDwhCwV2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprime el contenido de una p치gina.\n",
        "print(pdf_pages[0].page_content)"
      ],
      "metadata": {
        "id": "MSEhtS_VwudJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "6pYdyGPWxh2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de informaci칩n de p치gina web"
      ],
      "metadata": {
        "id": "iVJ1zw6bzQiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se carga la informaci칩n de una p치gina web.\n",
        "\n",
        "page_url = \"https://dialektico.com/cama-ultra-lujosa-para-gatos-dialektiroyal-comfort/\"\n",
        "\n",
        "loader = WebBaseLoader(web_paths=[page_url])\n",
        "docs = []\n",
        "async for doc in loader.alazy_load():\n",
        "    docs.append(doc)"
      ],
      "metadata": {
        "id": "KeMaLYkxzngJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprime la informaci칩n recabada.\n",
        "docs"
      ],
      "metadata": {
        "id": "xdVA4-Xt0K_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para conocer m치s opciones de carga de informaci칩n, puede consultarse:\n",
        "https://python.langchain.com/docs/concepts/text_splitters/\n",
        "\n"
      ],
      "metadata": {
        "id": "JLMI63490lNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "uf8qL1N51YN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dividir documentos\n",
        "\n",
        "Despu칠s de haber cargado la informaci칩n se debe realizar una divisi칩n de estos en fragmentos informativos m치s peque침os.\n",
        "\n",
        "**쯇or qu칠 dividir documentos?**\n",
        "Existen varias razones para dividir documentos:\n",
        "\n",
        "* **Manejo de longitudes no uniformes**: Las colecciones de documentos del mundo real suelen contener textos de diferentes tama침os. Dividirlos garantiza un procesamiento consistente entre todos los documentos.\n",
        "\n",
        "* **Superar las limitaciones del modelo**: Muchos modelos de embeddings y modelos de lenguaje tienen restricciones en el tama침o m치ximo de entrada. Dividir permite procesar documentos que, de otro modo, exceder칤an esos l칤mites.\n",
        "\n",
        "* **Mejorar la calidad de la representaci칩n**: En documentos largos, la calidad de los embeddings u otras representaciones puede degradarse al intentar capturar demasiada informaci칩n. Dividirlos puede generar representaciones m치s precisas y enfocadas de cada secci칩n.\n",
        "\n",
        "* **Aumentar la precisi칩n en la recuperaci칩n de informaci칩n**: En sistemas de recuperaci칩n de informaci칩n, dividir documentos puede mejorar la granularidad de los resultados, permitiendo una correspondencia m치s precisa entre consultas y secciones relevantes del documento.\n",
        "\n",
        "* **Optimizar los recursos computacionales**: Trabajar con fragmentos m치s peque침os de texto puede ser m치s eficiente en memoria y permite una mejor paralelizaci칩n de las tareas de procesamiento."
      ],
      "metadata": {
        "id": "jSEt65PR1_Qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracci칩n basada en longitud"
      ],
      "metadata": {
        "id": "6Ax9BGYQ2U4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La estrategia m치s intuitiva es dividir los documentos en funci칩n de su longitud. Este enfoque, simple pero eficaz, garantiza que cada fragmento no supere un l칤mite de tama침o especificado. Los principales beneficios de la divisi칩n basada en longitud son:\n",
        "\n",
        "* Implementaci칩n sencilla\n",
        "\n",
        "* Tama침os de fragmento consistentes\n",
        "\n",
        "* F치cil adaptaci칩n a los requisitos de diferentes modelos\n",
        "\n",
        "Tipos de divisi칩n basada en longitud:\n",
        "* Basada en tokens: Divide el texto seg칰n el n칰mero de tokens, lo cual es 칰til al trabajar con modelos de lenguaje.\n",
        "\n",
        "* Basada en caracteres: Divide el texto seg칰n el n칰mero de caracteres, lo cual puede ser m치s consistente entre distintos tipos de texto."
      ],
      "metadata": {
        "id": "VC7gbXR-2rTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divisi칩n basada en tokens"
      ],
      "metadata": {
        "id": "aG3XlB5F21bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se divide el texto utilizando el tokenizador tiktoken.\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name=\"cl100k_base\", chunk_size=1000, chunk_overlap=200\n",
        ")\n",
        "texts = text_splitter.split_documents(pdf_pages)"
      ],
      "metadata": {
        "id": "NzKmMvvd206C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0].page_content"
      ],
      "metadata": {
        "id": "u4E1e_lD3YQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Se ha dividido la informaci칩n en {len(texts)} documentos (chunks).\")"
      ],
      "metadata": {
        "id": "a2jfumxA34ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Divisi칩n basada en caracteres y estructura del texto"
      ],
      "metadata": {
        "id": "akAi89rQ48uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El texto est치 naturalmente organizado en unidades jer치rquicas como p치rrafos, oraciones y palabras. Podemos aprovechar esta estructura inherente para definir una estrategia de divisi칩n que mantenga el flujo natural del lenguaje, preserve la coherencia sem치ntica dentro de cada fragmento y se adapte a distintos niveles de granularidad del texto. El RecursiveCharacterTextSplitter de LangChain implementa este concepto:\n",
        "\n",
        "* RecursiveCharacterTextSplitter intenta mantener intactas las unidades m치s grandes (por ejemplo, los p치rrafos).\n",
        "\n",
        "* Si una unidad supera el tama침o m치ximo del fragmento, pasa al siguiente nivel (por ejemplo, las oraciones).\n",
        "\n",
        "* Este proceso contin칰a hasta el nivel de las palabras, si es necesario.\n"
      ],
      "metadata": {
        "id": "6KCuQwCm5XNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "texts = text_splitter.split_documents(pdf_pages)"
      ],
      "metadata": {
        "id": "mVHEsUaj5OqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0].page_content"
      ],
      "metadata": {
        "id": "iBjgbQLu6bYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Se ha dividido la informaci칩n en {len(texts)} sub-documentos.\")"
      ],
      "metadata": {
        "id": "LRNUWnCLyJ1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diferencia entre dividir por tokens y caracteres\n",
        "\n",
        "游댟 Divisi칩n por caracteres:\n",
        "* Se basa en la cantidad de letras, espacios y s칤mbolos que contiene el texto.\n",
        "* Es una forma muy directa y uniforme de dividir.\n",
        "* No considera palabras completas, estructuras ling칲칤sticas ni unidades sem치nticas.\n",
        "\n",
        "Ejemplo: \"Hola, 쯖칩mo est치s?\" tiene 18 caracteres (incluyendo signos y espacios).\n",
        "\n",
        "游빔 Divisi칩n por tokens:\n",
        "* Se basa en el n칰mero de tokens generados por un tokenizador (como el de OpenAI o HuggingFace).\n",
        "* Un token no siempre es igual a una palabra: A veces una sola palabra genera 2 o m치s tokens. Palabras muy comunes generan menos tokens.\n",
        "* Es 칰til cuando trabajas con modelos LLM porque ellos procesan tokens, no caracteres."
      ],
      "metadata": {
        "id": "Eb4kbvTU85xS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "쮺u치l elegir?\n",
        "\n",
        "<br>\n",
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2025/03/tabla_cursoRAG_2.png\" width=\"800\" /></center>"
      ],
      "metadata": {
        "id": "cpMWSYiX7Tl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "M치s informaci칩n en:\n",
        "- https://python.langchain.com/api_reference/text_splitters/index.html\n",
        "- https://python.langchain.com/docs/concepts/text_splitters/"
      ],
      "metadata": {
        "id": "nJaVkNUfM4hz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "IxeT_AvA3ERH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dialektico Machine learning practices 춸 2025 by Daniel Antonio Garc칤a Escobar\n",
        "# is licensed under CC BY-NC 4.0. To view a copy of this license,\n",
        "# visit https://creativecommons.org/licenses/by-nc/4.0/\n",
        "\n",
        "# Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\n",
        "# Public License"
      ],
      "metadata": {
        "id": "7ly3JFP2WEOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}