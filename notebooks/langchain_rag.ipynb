{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpfOKHQOkUXFCZ+l2wLuju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielDialektico/rag_agentes_langchain_curso/blob/main/notebooks/langchain_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://dialektico.com/wp-content/uploads/2023/03/MiniLogoW4.png\" alt=\"Dial√©ktico Logo\" />"
      ],
      "metadata": {
        "id": "Vc-8Grt7ORmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este peque√±o tutorial pertenece al curso de RAG y agentes con LangChain al que puedes acceder mediante la siguiente URL: https://www.youtube.com/playlist?list=PLlWTv9_GeWd32stuEMWpYOnxiVxnXaU6q\n",
        "\n",
        "Sigue los videos del curso para recibir instrucciones y contexto sobre la ejecuci√≥n de este Notebook."
      ],
      "metadata": {
        "id": "FdVKmZZtOT3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "ad-poWzqOWWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Se instalan e importan las librer√≠as"
      ],
      "metadata": {
        "id": "qFIVZvp0OZ2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se instalan las librer√≠as.\n",
        "!pip install langchain-pinecone==0.2.4\n",
        "!pip install langchain==0.3.21\n",
        "!pip install langchain-community==0.3.20\n",
        "!pip install beautifulsoup4==4.13.3\n",
        "!pip install tiktoken==0.9.0\n",
        "!pip install pypdf==5.4.0\n",
        "!pip install pinecone==6.0.2\n",
        "!pip install langchain-huggingface==0.1.2\n",
        "!pip install langchain_deepseek==0.1.2"
      ],
      "metadata": {
        "id": "8U_GtE_0tuGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dTpru6JOFTj"
      },
      "outputs": [],
      "source": [
        "# Se importan las librer√≠as.\n",
        "import os\n",
        "import warnings\n",
        "import sys\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.schema import BaseChatMessageHistory, BaseMessage\n",
        "import bs4\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from uuid import uuid4\n",
        "from IPython.display import display, HTML\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "-FoCFQISWQSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorizaci√≥n de documentos y almacenamiento."
      ],
      "metadata": {
        "id": "4lxITC5Y8I9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se carga la informaci√≥n desde distintas fuentes\n",
        "web_loader = WebBaseLoader(web_paths=[\"https://dialektico.com/cama-ultra-lujosa-para-gatos-dialektiroyal-comfort/\"])\n",
        "documents = []\n",
        "\n",
        "for doc in os.listdir('documents'):\n",
        "  pdf_loader = PyPDFLoader('documents/'+ doc)\n",
        "  async for page in pdf_loader.alazy_load():\n",
        "      documents.append(page)\n",
        "\n",
        "async for doc in web_loader.alazy_load():\n",
        "    documents.append(doc)"
      ],
      "metadata": {
        "id": "hIuxRdvTB71L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de documentos.\n",
        "len(documents)"
      ],
      "metadata": {
        "id": "erHBgYXcuKSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se prepara el algoritmo de embeddings a utilizar.\n",
        "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
      ],
      "metadata": {
        "id": "TCiSMQ4wCyzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea vector store en memoria para almacenar vectores.\n",
        "vector_store = InMemoryVectorStore(embedding=embeddings_model)\n",
        "\n",
        "# Se instancia el CharacterTextSplitter.\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name=\"cl100k_base\", chunk_size=1000, chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Se divide el texto de los documentos utilizando el tokenizador tiktoken.\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Se almacenan los vectores.\n",
        "vector_store.add_documents(documents=chunks, ids=[str(uuid4()) for _ in range(len(chunks))])"
      ],
      "metadata": {
        "id": "uWtT1zcCqD2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "NB4NpfRX8VuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se a√±ade la API key como variable de ambiente desde un secreto en Colab.\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = userdata.get('DEEPSEEK_API_KEY')\n",
        "\n",
        "# Se define el modelo a utilizar y a√±aden valores de par√°metros.\n",
        "model = ChatDeepSeek(\n",
        "      model=\"deepseek-chat\",\n",
        "      temperature=0,\n",
        "      max_tokens=200\n",
        "      )"
      ],
      "metadata": {
        "id": "GFtPjHmF7qBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea template para RAG.\n",
        "template = \"\"\"Utiliza los siguientes fragmentos de contexto para responder la\n",
        "pregunta al final. Si no sabes la respuesta, simplemente di que no la sabes, no\n",
        "intentes inventar una respuesta.\n",
        "Utiliza pocas oraciones y mant√©n la respuesta lo m√°s concisa posible.\n",
        "\n",
        "CONTEXTO: {context}\n",
        "\n",
        "PREGUNTA: {question}\n",
        "\n",
        "Respuesta √∫til:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "message = prompt.invoke(\n",
        "    {\"context\": \"(Se a√±ade el contexto)\", \"question\": \"(Se a√±ade la pregunta)\"}\n",
        ").to_messages()\n",
        "\n",
        "print(message[0].content)"
      ],
      "metadata": {
        "id": "t_I3tKo5uyaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recuperaci√≥n (retrieve)"
      ],
      "metadata": {
        "id": "TvAiD_nU8-4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se traen documentos con mayor similaridad.\n",
        "question = \"¬øQu√© incluye la cama ultralujosa para gatos?\"\n",
        "retrieved_docs = vector_store.similarity_search(question, k=5)\n",
        "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)"
      ],
      "metadata": {
        "id": "7aeucX8D5TCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_content"
      ],
      "metadata": {
        "id": "lOSqtwbS5_HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Incremento (Augment)"
      ],
      "metadata": {
        "id": "gIFlu7r99lbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se a√±ade la informaci√≥n al prompt.\n",
        "messages = prompt.invoke({\"question\": question, \"context\": docs_content})"
      ],
      "metadata": {
        "id": "5Ib_mzlPGJAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(messages)"
      ],
      "metadata": {
        "id": "3D8w0WR25oT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generaci√≥n (Generation)"
      ],
      "metadata": {
        "id": "UB-QTHdw9roP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se genera la respuesta con el contexto dado.\n",
        "response = model.invoke(messages)\n",
        "response.content"
      ],
      "metadata": {
        "id": "SPLoxo7gsxq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "3eikvF_w8PQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se encapsula la l√≥gica en una sola funci√≥n para crear un chat."
      ],
      "metadata": {
        "id": "3NIMqL5b9wDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rag_chat(vector_store, model):\n",
        "\n",
        "    # Se define el template\n",
        "    prompt_template = \"\"\"Utiliza los siguientes fragmentos de contexto para responder la\n",
        "    pregunta al final. Si no sabes la respuesta, simplemente di que no la sabes, no\n",
        "    intentes inventar una respuesta.\n",
        "    Utiliza pocas oraciones y mant√©n la respuesta lo m√°s concisa posible.\n",
        "\n",
        "    CONTEXTO: {context}\n",
        "\n",
        "    PREGUNTA: {question}\n",
        "\n",
        "    Respuesta √∫til:\"\"\"\n",
        "\n",
        "    print(\"\\nüìù Escribe una pregunta (o escribe 'salir' para terminar):\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_question = input(\"\\nPregunta: \").strip()\n",
        "\n",
        "            if user_question.lower() in ('salir', 'exit', 'quit'):\n",
        "                print(\"\\nüö™ Saliendo del chat. ¬°Hasta luego!\")\n",
        "                break\n",
        "\n",
        "            # Se recuperan documentos similares.\n",
        "            retrieved_docs = vector_store.similarity_search(user_question, k=5)\n",
        "            docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "\n",
        "            # Se a√±ade el contexto al prompt.\n",
        "            prompt = PromptTemplate.from_template(prompt_template)\n",
        "            messages = prompt.invoke({\"question\": user_question, \"context\": docs_content})\n",
        "\n",
        "            # Se genera la respuesta.\n",
        "            response = model.invoke(messages)\n",
        "\n",
        "            # Se muestra la respuesta.\n",
        "            display(HTML(f\"<div style='padding:10px; border:1px solid #ccc; border-radius:8px;'><b>Respuesta:</b><br>{response.content}</div>\"))\n",
        "\n",
        "            print(\"\\nüìù Escribe otra pregunta (o 'salir' para terminar):\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nüõë Interrupci√≥n manual detectada. Cerrando chat.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùó Error: {e}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "qyzqg6_b-ZYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_rag_chat(vector_store, model)"
      ],
      "metadata": {
        "id": "wbOI3i9M-vME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "a34sxv6UCSuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se a√±ade memoria y streaming al chat."
      ],
      "metadata": {
        "id": "Ej0k19Sx91qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rag_chat_memory(vector_store, model):\n",
        "\n",
        "    # Definir historial de mensajes en memoria.\n",
        "    class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
        "        messages: List[BaseMessage] = Field(default_factory=list)\n",
        "\n",
        "        def add_messages(self, messages: List[BaseMessage]) -> None:\n",
        "            self.messages.extend(messages)\n",
        "\n",
        "        def clear(self) -> None:\n",
        "            self.messages = []\n",
        "\n",
        "    store = {}\n",
        "\n",
        "    def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
        "        if session_id not in store:\n",
        "            store[session_id] = InMemoryHistory()\n",
        "        return store[session_id]\n",
        "\n",
        "    # Definir el template dentro de la funci√≥n\n",
        "    prompt_template = \"\"\"Utiliza los siguientes fragmentos de contexto para responder la\n",
        "    pregunta al final. Si no sabes la respuesta, simplemente di que no la sabes, no\n",
        "    intentes inventar una respuesta.\n",
        "    Utiliza pocas oraciones y mant√©n la respuesta lo m√°s concisa posible.\n",
        "\n",
        "    CONTEXTO: {context}\n",
        "\n",
        "    PREGUNTA: {question}\n",
        "\n",
        "    Respuesta √∫til:\"\"\"\n",
        "\n",
        "    # Se genera un un session_id din√°mico.\n",
        "    session_id = str(uuid4())\n",
        "    print(f\"\\nüÜî Sesi√≥n iniciada con ID: {session_id}\")\n",
        "    print(\"üìù Escribe una pregunta (o escribe 'salir' para terminar):\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_question = input(\"\\nPregunta: \").strip()\n",
        "\n",
        "            if user_question.lower() in ('salir', 'exit', 'quit'):\n",
        "                print(\"\\nüö™ Saliendo del chat. ¬°Hasta luego!\")\n",
        "                break\n",
        "\n",
        "            # Se recuperan documentos similares.\n",
        "            retrieved_docs = vector_store.similarity_search(user_question, k=5)\n",
        "            docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "\n",
        "            # Crear el prompt compuesto\n",
        "            chat_prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"system\", prompt_template),\n",
        "                MessagesPlaceholder(variable_name=\"history\"),\n",
        "                (\"user\", \"{question}\"),\n",
        "            ])\n",
        "\n",
        "            # Encadenar prompt con modelo\n",
        "            chain = chat_prompt | model\n",
        "\n",
        "            # A√±adir historial\n",
        "            chain_with_history = RunnableWithMessageHistory(\n",
        "                chain,\n",
        "                get_by_session_id,\n",
        "                input_messages_key=\"question\",\n",
        "                history_messages_key=\"history\"\n",
        "            )\n",
        "\n",
        "            # Se aumenta y genera la respuesta, mostr√°ndose con streaming.\n",
        "            print(\"\\nüí¨ Respuesta:\")\n",
        "            chunks = []\n",
        "            for chunk in chain_with_history.stream(\n",
        "                {\"question\": user_question, \"context\": docs_content},\n",
        "                config={\"configurable\": {\"session_id\": session_id}}\n",
        "            ):\n",
        "                chunks.append(chunk)\n",
        "                print(chunk.content, end=\"\", flush=True)\n",
        "\n",
        "            print(\"\\n\\nüìù Escribe otra pregunta (o 'salir' para terminar):\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nüõë Interrupci√≥n manual detectada. Cerrando chat.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùó Error: {e}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "w1DHytPoCT_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_rag_chat_memory(vector_store, model)"
      ],
      "metadata": {
        "id": "S8JhvfPKDegU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dialektico Machine learning practices ¬© 2025 by Daniel Antonio Garc√≠a Escobar\n",
        "# is licensed under CC BY-NC 4.0. To view a copy of this license,\n",
        "# visit https://creativecommons.org/licenses/by-nc/4.0/\n",
        "\n",
        "# Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\n",
        "# Public License"
      ],
      "metadata": {
        "id": "7ly3JFP2WEOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}