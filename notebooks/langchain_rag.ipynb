{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpfOKHQOkUXFCZ+l2wLuju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielDialektico/rag_agentes_langchain_curso/blob/main/notebooks/langchain_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://dialektico.com/wp-content/uploads/2023/03/MiniLogoW4.png\" alt=\"Dialéktico Logo\" />"
      ],
      "metadata": {
        "id": "Vc-8Grt7ORmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este pequeño tutorial pertenece al curso de RAG y agentes con LangChain al que puedes acceder mediante la siguiente URL: https://www.youtube.com/playlist?list=PLlWTv9_GeWd32stuEMWpYOnxiVxnXaU6q\n",
        "\n",
        "Sigue los videos del curso para recibir instrucciones y contexto sobre la ejecución de este Notebook."
      ],
      "metadata": {
        "id": "FdVKmZZtOT3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "ad-poWzqOWWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Se instalan e importan las librerías"
      ],
      "metadata": {
        "id": "qFIVZvp0OZ2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se instalan las librerías.\n",
        "!pip install langchain-pinecone==0.2.4\n",
        "!pip install langchain==0.3.21\n",
        "!pip install langchain-community==0.3.20\n",
        "!pip install beautifulsoup4==4.13.3\n",
        "!pip install tiktoken==0.9.0\n",
        "!pip install pypdf==5.4.0\n",
        "!pip install pinecone==6.0.2\n",
        "!pip install langchain-huggingface==0.1.2\n",
        "!pip install langchain_deepseek==0.1.2"
      ],
      "metadata": {
        "id": "8U_GtE_0tuGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dTpru6JOFTj"
      },
      "outputs": [],
      "source": [
        "# Se importan las librerías.\n",
        "import os\n",
        "import warnings\n",
        "import sys\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.schema import BaseChatMessageHistory, BaseMessage\n",
        "import bs4\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from uuid import uuid4\n",
        "from IPython.display import display, HTML\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "-FoCFQISWQSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorización de documentos y almacenamiento."
      ],
      "metadata": {
        "id": "4lxITC5Y8I9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se carga la información desde distintas fuentes\n",
        "web_loader = WebBaseLoader(web_paths=[\"https://dialektico.com/cama-ultra-lujosa-para-gatos-dialektiroyal-comfort/\"])\n",
        "documents = []\n",
        "\n",
        "for doc in os.listdir('documents'):\n",
        "  pdf_loader = PyPDFLoader('documents/'+ doc)\n",
        "  async for page in pdf_loader.alazy_load():\n",
        "      documents.append(page)\n",
        "\n",
        "async for doc in web_loader.alazy_load():\n",
        "    documents.append(doc)"
      ],
      "metadata": {
        "id": "hIuxRdvTB71L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de documentos.\n",
        "len(documents)"
      ],
      "metadata": {
        "id": "erHBgYXcuKSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se prepara el algoritmo de embeddings a utilizar.\n",
        "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
      ],
      "metadata": {
        "id": "TCiSMQ4wCyzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea vector store en memoria para almacenar vectores.\n",
        "vector_store = InMemoryVectorStore(embedding=embeddings_model)\n",
        "\n",
        "# Se instancia el CharacterTextSplitter.\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name=\"cl100k_base\", chunk_size=1000, chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Se divide el texto de los documentos utilizando el tokenizador tiktoken.\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Se almacenan los vectores.\n",
        "vector_store.add_documents(documents=chunks, ids=[str(uuid4()) for _ in range(len(chunks))])"
      ],
      "metadata": {
        "id": "uWtT1zcCqD2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "NB4NpfRX8VuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se añade la API key como variable de ambiente desde un secreto en Colab.\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = userdata.get('DEEPSEEK_API_KEY')\n",
        "\n",
        "# Se define el modelo a utilizar y añaden valores de parámetros.\n",
        "model = ChatDeepSeek(\n",
        "      model=\"deepseek-chat\",\n",
        "      temperature=0,\n",
        "      max_tokens=200\n",
        "      )"
      ],
      "metadata": {
        "id": "GFtPjHmF7qBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea template para RAG.\n",
        "template = \"\"\"Utiliza los siguientes fragmentos de contexto para responder la\n",
        "pregunta al final. Si no sabes la respuesta, simplemente di que no la sabes, no\n",
        "intentes inventar una respuesta.\n",
        "Utiliza pocas oraciones y mantén la respuesta lo más concisa posible.\n",
        "\n",
        "CONTEXTO: {context}\n",
        "\n",
        "PREGUNTA: {question}\n",
        "\n",
        "Respuesta útil:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "message = prompt.invoke(\n",
        "    {\"context\": \"(Se añade el contexto)\", \"question\": \"(Se añade la pregunta)\"}\n",
        ").to_messages()\n",
        "\n",
        "print(message[0].content)"
      ],
      "metadata": {
        "id": "t_I3tKo5uyaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recuperación (retrieve)"
      ],
      "metadata": {
        "id": "TvAiD_nU8-4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se traen documentos con mayor similaridad.\n",
        "question = \"¿Qué incluye la cama ultralujosa para gatos?\"\n",
        "retrieved_docs = vector_store.similarity_search(question, k=5)\n",
        "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)"
      ],
      "metadata": {
        "id": "7aeucX8D5TCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_content"
      ],
      "metadata": {
        "id": "lOSqtwbS5_HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Incremento (Augment)"
      ],
      "metadata": {
        "id": "gIFlu7r99lbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se añade la información al prompt.\n",
        "messages = prompt.invoke({\"question\": question, \"context\": docs_content})"
      ],
      "metadata": {
        "id": "5Ib_mzlPGJAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(messages)"
      ],
      "metadata": {
        "id": "3D8w0WR25oT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación (Generation)"
      ],
      "metadata": {
        "id": "UB-QTHdw9roP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se genera la respuesta con el contexto dado.\n",
        "response = model.invoke(messages)\n",
        "response.content"
      ],
      "metadata": {
        "id": "SPLoxo7gsxq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "3eikvF_w8PQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se encapsula la lógica en una sola función para crear un chat."
      ],
      "metadata": {
        "id": "3NIMqL5b9wDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rag_chat(vector_store, model):\n",
        "\n",
        "    # Se define el template\n",
        "    prompt_template = \"\"\"Utiliza los siguientes fragmentos de contexto para responder la\n",
        "    pregunta al final. Si no sabes la respuesta, simplemente di que no la sabes, no\n",
        "    intentes inventar una respuesta.\n",
        "    Utiliza pocas oraciones y mantén la respuesta lo más concisa posible.\n",
        "\n",
        "    CONTEXTO: {context}\n",
        "\n",
        "    PREGUNTA: {question}\n",
        "\n",
        "    Respuesta útil:\"\"\"\n",
        "\n",
        "    print(\"\\n📝 Escribe una pregunta (o escribe 'salir' para terminar):\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_question = input(\"\\nPregunta: \").strip()\n",
        "\n",
        "            if user_question.lower() in ('salir', 'exit', 'quit'):\n",
        "                print(\"\\n🚪 Saliendo del chat. ¡Hasta luego!\")\n",
        "                break\n",
        "\n",
        "            # Se recuperan documentos similares.\n",
        "            retrieved_docs = vector_store.similarity_search(user_question, k=5)\n",
        "            docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "\n",
        "            # Se añade el contexto al prompt.\n",
        "            prompt = PromptTemplate.from_template(prompt_template)\n",
        "            messages = prompt.invoke({\"question\": user_question, \"context\": docs_content})\n",
        "\n",
        "            # Se genera la respuesta.\n",
        "            response = model.invoke(messages)\n",
        "\n",
        "            # Se muestra la respuesta.\n",
        "            display(HTML(f\"<div style='padding:10px; border:1px solid #ccc; border-radius:8px;'><b>Respuesta:</b><br>{response.content}</div>\"))\n",
        "\n",
        "            print(\"\\n📝 Escribe otra pregunta (o 'salir' para terminar):\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n🛑 Interrupción manual detectada. Cerrando chat.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❗ Error: {e}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "qyzqg6_b-ZYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_rag_chat(vector_store, model)"
      ],
      "metadata": {
        "id": "wbOI3i9M-vME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "a34sxv6UCSuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se añade memoria y streaming al chat."
      ],
      "metadata": {
        "id": "Ej0k19Sx91qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rag_chat_memory(vector_store, model):\n",
        "\n",
        "    # Definir historial de mensajes en memoria.\n",
        "    class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
        "        messages: List[BaseMessage] = Field(default_factory=list)\n",
        "\n",
        "        def add_messages(self, messages: List[BaseMessage]) -> None:\n",
        "            self.messages.extend(messages)\n",
        "\n",
        "        def clear(self) -> None:\n",
        "            self.messages = []\n",
        "\n",
        "    store = {}\n",
        "\n",
        "    def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
        "        if session_id not in store:\n",
        "            store[session_id] = InMemoryHistory()\n",
        "        return store[session_id]\n",
        "\n",
        "    # Definir el template dentro de la función\n",
        "    prompt_template = \"\"\"Utiliza los siguientes fragmentos de contexto para responder la\n",
        "    pregunta al final. Si no sabes la respuesta, simplemente di que no la sabes, no\n",
        "    intentes inventar una respuesta.\n",
        "    Utiliza pocas oraciones y mantén la respuesta lo más concisa posible.\n",
        "\n",
        "    CONTEXTO: {context}\n",
        "\n",
        "    PREGUNTA: {question}\n",
        "\n",
        "    Respuesta útil:\"\"\"\n",
        "\n",
        "    # Se genera un un session_id dinámico.\n",
        "    session_id = str(uuid4())\n",
        "    print(f\"\\n🆔 Sesión iniciada con ID: {session_id}\")\n",
        "    print(\"📝 Escribe una pregunta (o escribe 'salir' para terminar):\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_question = input(\"\\nPregunta: \").strip()\n",
        "\n",
        "            if user_question.lower() in ('salir', 'exit', 'quit'):\n",
        "                print(\"\\n🚪 Saliendo del chat. ¡Hasta luego!\")\n",
        "                break\n",
        "\n",
        "            # Se recuperan documentos similares.\n",
        "            retrieved_docs = vector_store.similarity_search(user_question, k=5)\n",
        "            docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "\n",
        "            # Crear el prompt compuesto\n",
        "            chat_prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"system\", prompt_template),\n",
        "                MessagesPlaceholder(variable_name=\"history\"),\n",
        "                (\"user\", \"{question}\"),\n",
        "            ])\n",
        "\n",
        "            # Encadenar prompt con modelo\n",
        "            chain = chat_prompt | model\n",
        "\n",
        "            # Añadir historial\n",
        "            chain_with_history = RunnableWithMessageHistory(\n",
        "                chain,\n",
        "                get_by_session_id,\n",
        "                input_messages_key=\"question\",\n",
        "                history_messages_key=\"history\"\n",
        "            )\n",
        "\n",
        "            # Se aumenta y genera la respuesta, mostrándose con streaming.\n",
        "            print(\"\\n💬 Respuesta:\")\n",
        "            chunks = []\n",
        "            for chunk in chain_with_history.stream(\n",
        "                {\"question\": user_question, \"context\": docs_content},\n",
        "                config={\"configurable\": {\"session_id\": session_id}}\n",
        "            ):\n",
        "                chunks.append(chunk)\n",
        "                print(chunk.content, end=\"\", flush=True)\n",
        "\n",
        "            print(\"\\n\\n📝 Escribe otra pregunta (o 'salir' para terminar):\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n🛑 Interrupción manual detectada. Cerrando chat.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❗ Error: {e}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "w1DHytPoCT_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_rag_chat_memory(vector_store, model)"
      ],
      "metadata": {
        "id": "S8JhvfPKDegU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dialektico Machine learning practices © 2025 by Daniel Antonio García Escobar\n",
        "# is licensed under CC BY-NC 4.0. To view a copy of this license,\n",
        "# visit https://creativecommons.org/licenses/by-nc/4.0/\n",
        "\n",
        "# Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\n",
        "# Public License"
      ],
      "metadata": {
        "id": "7ly3JFP2WEOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}