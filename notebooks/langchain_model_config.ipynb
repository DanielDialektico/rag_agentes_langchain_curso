{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4A1lUbHOYxkIBDYwmlPJr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielDialektico/rag_agentes_langchain_curso/blob/main/notebooks/langchain_model_config.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://dialektico.com/wp-content/uploads/2023/03/MiniLogoW4.png\" alt=\"Dialéktico Logo\" />"
      ],
      "metadata": {
        "id": "Vc-8Grt7ORmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este pequeño tutorial pertenece al curso de RAG y agentes con LangChain al que puedes acceder mediante la siguiente URL: https://www.youtube.com/playlist?list=PLlWTv9_GeWd32stuEMWpYOnxiVxnXaU6q\n",
        "\n",
        "Sigue los videos del curso para recibir instrucciones y contexto sobre la ejecución de este Notebook."
      ],
      "metadata": {
        "id": "FdVKmZZtOT3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "ad-poWzqOWWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Se instalan e importan las librerías"
      ],
      "metadata": {
        "id": "qFIVZvp0OZ2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dTpru6JOFTj"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.3.20\n",
        "!pip install langchain_deepseek==0.1.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from google.colab import userdata\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "1-fGXHRU-p3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se añade valor de API key mediante un secreto"
      ],
      "metadata": {
        "id": "Uslpy62GOh3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se añade la API key como variable de ambiente desde un secreto en Colab.\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = userdata.get('DEEPSEEK_API_KEY')"
      ],
      "metadata": {
        "id": "HitB-RQEOiqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se declara el modelo a utilizar"
      ],
      "metadata": {
        "id": "2xjS3Gb7Ov6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define el modelo.\n",
        "model = ChatDeepSeek(model=\"deepseek-chat\")"
      ],
      "metadata": {
        "id": "Vn5osNj5OxgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "0POu3P-KQw_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temperatura"
      ],
      "metadata": {
        "id": "G6F1vx7SQwIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La temperatura controla la aleatoriedad de las respuestas generadas. Valores más bajos producen respuestas más deterministas, mientras que valores más altos introducen mayor variabilidad (creatividad)."
      ],
      "metadata": {
        "id": "BglKOhiyQ0OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define el modelo y añaden valores de parámetros.\n",
        "model = ChatDeepSeek(\n",
        "      model=\"deepseek-chat\",\n",
        "      temperature=0\n",
        "      )\n",
        "# Se añade una prompt y se imprime la respuesta.\n",
        "response = model.invoke(\"¿Qué es un automóvil?\")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "id": "0SViFiwRQ8UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define el modelo y añaden valores de parámetros.\n",
        "model = ChatDeepSeek(\n",
        "      model=\"deepseek-chat\",\n",
        "      temperature=1\n",
        "      )\n",
        "\n",
        "# Se añade una prompt y se imprime la respuesta.\n",
        "response = model.invoke(\"¿Qué es un automóvil?\")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "id": "7moEi9M3Rlzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://dialektico.com/wp-content/uploads/2025/03/Tab_COB1.png\" alt=\"\" /></center>"
      ],
      "metadata": {
        "id": "kXSQ0WxnS49w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "PcvT_7g7T-5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metadatos de la respuesta."
      ],
      "metadata": {
        "id": "ZUheszXtUyDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.usage_metadata"
      ],
      "metadata": {
        "id": "KsMjNqCoSqzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "TP88n6cSVLsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El contenido de `response.usage_metadata` muestra información sobre el uso de tokens en una respuesta generada por un modelo de lenguaje como **DeepSeek** (o cualquier otro modelo compatible con LangChain). Aquí está el desglose de cada clave:\n",
        "\n",
        "\n",
        "- **`input_tokens`: 11**  \n",
        "  - Indica la cantidad de **tokens** que fueron utilizados en la consulta del usuario (*prompt*).  \n",
        "  - Un token puede ser una palabra, parte de una palabra o un carácter en algunos casos.  \n",
        "  - En este caso, la consulta fue corta (**11 tokens**).\n",
        "\n",
        "- **`output_tokens`: 334**  \n",
        "  - Representa el número de **tokens generados por el modelo** como respuesta.  \n",
        "  - En este caso, el modelo generó **334 tokens** en su respuesta.\n",
        "\n",
        "- **`total_tokens`: 345**  \n",
        "  - Es la **suma de `input_tokens` y `output_tokens`** (11 + 334 = **345**).  \n",
        "  - Esto indica la cantidad total de tokens procesados en la solicitud.\n",
        "\n",
        "- **`input_token_details`: `{'cache_read': 0}`**  \n",
        "  - Muestra detalles sobre cómo se manejaron los **tokens de entrada**.  \n",
        "  - `cache_read: 0` indica que **ninguna parte de la solicitud se obtuvo desde caché**, es decir, el modelo procesó el *input* en tiempo real.\n",
        "\n",
        "- **`output_token_details`: `{}`**  \n",
        "  - Contendría detalles adicionales sobre los tokens generados, pero en este caso está vacío.\n",
        "\n",
        "---\n",
        "\n",
        "**¿Por qué es relevante esta información?**\n",
        "- **Optimización de costos:** Si el modelo se está ejecutando en una API de pago, el costo suele depender del número de tokens usados.\n",
        "- **Rendimiento:** Una alta cantidad de `output_tokens` puede indicar respuestas largas, lo que puede necesitar ajuste con `max_tokens`.\n",
        "- **Caching:** Si `cache_read` fuera mayor a 0, significaría que se reutilizó información previa, reduciendo el tiempo de respuesta."
      ],
      "metadata": {
        "id": "0WZBI6C7Ubxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "r5mPpsJOVYQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokens máximos"
      ],
      "metadata": {
        "id": "Jttu2tADVZBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este parámetro configura el número máximo de tokens que se generarán en la respuesta."
      ],
      "metadata": {
        "id": "G1-T95pXVjPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define el modelo y añaden valores de parámetros.\n",
        "model = ChatDeepSeek(\n",
        "      model=\"deepseek-chat\",\n",
        "      temperature=0,\n",
        "      max_tokens=100\n",
        "      )\n",
        "# Se añade una prompt y se imprime la respuesta.\n",
        "response = model.invoke(\"¿Qué es un automóvil?\")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "id": "FtJlWpumUJ2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprimen los metadatos de las respuestas.\n",
        "response.usage_metadata"
      ],
      "metadata": {
        "id": "fmXXt2AXVtzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "sDiOUjgDWDMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todas las configuraciones posibles pueden revisarse en: https://python.langchain.com/api_reference/deepseek/chat_models/langchain_deepseek.chat_models.ChatDeepSeek.html"
      ],
      "metadata": {
        "id": "uDy5clRWWH2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "-FoCFQISWQSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Información relevante:\n",
        "\n",
        "* Cómo configurar límites de tasa de uso de la API: https://python.langchain.com/docs/how_to/chat_model_rate_limiting/\n",
        "* Ajuste de recorte de mensajes: https://python.langchain.com/docs/how_to/trim_messages/"
      ],
      "metadata": {
        "id": "Qk48gO7RcfK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dialektico Machine learning practices © 2025 by Daniel Antonio García Escobar\n",
        "# is licensed under CC BY-NC 4.0. To view a copy of this license,\n",
        "# visit https://creativecommons.org/licenses/by-nc/4.0/\n",
        "\n",
        "# Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\n",
        "# Public License"
      ],
      "metadata": {
        "id": "7ly3JFP2WEOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}