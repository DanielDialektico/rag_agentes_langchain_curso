{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmA5EOpI8ceM4zz/6oDknO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielDialektico/rag_agentes_langchain_curso/blob/main/notebooks/langchain_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://dialektico.com/wp-content/uploads/2023/03/MiniLogoW4.png\" alt=\"Dialéktico Logo\" />"
      ],
      "metadata": {
        "id": "Vc-8Grt7ORmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este pequeño tutorial pertenece al curso de RAG y agentes con LangChain al que puedes acceder mediante la siguiente URL: https://www.youtube.com/playlist?list=PLlWTv9_GeWd32stuEMWpYOnxiVxnXaU6q\n",
        "\n",
        "Sigue los videos del curso para recibir instrucciones y contexto sobre la ejecución de este Notebook."
      ],
      "metadata": {
        "id": "FdVKmZZtOT3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "ad-poWzqOWWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Se instalan e importan las librerías"
      ],
      "metadata": {
        "id": "qFIVZvp0OZ2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dTpru6JOFTj"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.3.20\n",
        "!pip install langchain_deepseek==0.1.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "GJhIhrbOCGM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se añade valor de API key mediante un secreto"
      ],
      "metadata": {
        "id": "Uslpy62GOh3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se añade la API key como variable de ambiente desde un secreto en Colab.\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = userdata.get('DEEPSEEK_API_KEY')"
      ],
      "metadata": {
        "id": "HitB-RQEOiqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se declara el modelo a utilizar"
      ],
      "metadata": {
        "id": "2xjS3Gb7Ov6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define el modelo y añaden valores de parámetros.\n",
        "model = ChatDeepSeek(\n",
        "      model=\"deepseek-chat\",\n",
        "      temperature=0,\n",
        "      max_tokens=100\n",
        "      )"
      ],
      "metadata": {
        "id": "Vn5osNj5OxgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "-FoCFQISWQSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estructura de mensajes"
      ],
      "metadata": {
        "id": "uCbv2Q6kgnuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "​En LangChain, los mensajes se clasifican en diferentes tipos para estructurar y gestionar eficazmente las interacciones con los modelos. Los elementos son los siguientes:\n",
        "\n",
        "* **SystemMessage:** se utiliza para proporcionar instrucciones o contexto al modelo de inteligencia artificial, orientando su comportamiento en la conversación. Corresponde a un rol de sistema (system).\n",
        "\n",
        "* **HumanMessage:** El HumanMessage representa la entrada proporcionada por el usuario durante la interacción. Corresponde a un rol de usuario (user).\n",
        "\n",
        "* **AIMessage:** El AIMessage corresponde a la respuesta generada por el modelo de inteligencia artificial. Corresponde a un rol de asistente (assistant).\n",
        "\n",
        "* **AIMessageChunk**: se utiliza para stremear respuestas (en flujo). Corresponde a un rol de asistente (assistant).\n",
        "\n",
        "* **ToolMessage**: se utiliza para observar los resultados de llamadas de herramientas. Tiene un rol de herramienta (tool).\n",
        "\n",
        "Más información: https://python.langchain.com/docs/concepts/messages/"
      ],
      "metadata": {
        "id": "6F-pioVigp9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprime la estructura del mensaje.\n",
        "response = model.invoke('¡Hola!')\n",
        "response"
      ],
      "metadata": {
        "id": "bus980as5Y8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprime únicamente la respuesta.\n",
        "response.content"
      ],
      "metadata": {
        "id": "6Frpe_ClVC_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se añaden instrucciones a nivel system, y se inserta un prompt.\n",
        "messages = [\n",
        "    SystemMessage(\"Eres un asistente virtual llamado Dialectibot, saluda cordialmente diciendo tu nombre.\"),\n",
        "    HumanMessage(\"¡Hola!\"),\n",
        "]"
      ],
      "metadata": {
        "id": "A2tgtbHqgmWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se obtiene la respuesta del modelo.\n",
        "response = model.invoke(messages)\n",
        "response"
      ],
      "metadata": {
        "id": "NbMzEcyWPZLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se imprime únicamente la respuesta.\n",
        "response.content"
      ],
      "metadata": {
        "id": "TvguvCSPjcwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Más información sobre runnables en LangChain: https://python.langchain.com/docs/concepts/runnables/"
      ],
      "metadata": {
        "id": "By7r3HwAJPk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "2YSDJBdigk7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plantillas de prompts"
      ],
      "metadata": {
        "id": "-Qmy2M-XkFmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los templates de prompts ayudan a traducir la entrada del usuario y los parámetros en instrucciones para un modelo de lenguaje. Esto se puede utilizar para guiar la respuesta del modelo, ayudándolo a comprender el contexto y generar una salida relevante y coherente.\n",
        "\n",
        "Los templates de prompts reciben como entrada un diccionario, donde cada clave representa una variable dentro del template de prompt que debe completarse."
      ],
      "metadata": {
        "id": "QDyyi6c4kJZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-Shot prompting"
      ],
      "metadata": {
        "id": "gZnW_97B2jiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El Zero-Shot Prompting es una técnica en la que se le da solo una instrucción al modelo para generar una respuesta."
      ],
      "metadata": {
        "id": "QpZk7_5Y7_by"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En LanghChain existen templates de prompts que se pueden utilizar para dar formato a una sola cadena de texto."
      ],
      "metadata": {
        "id": "VA8f6YGj2_M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea una plantilla de prompts.\n",
        "prompt_template = PromptTemplate.from_template(\"Dime el nombre de un libro famoso de {genre}. Solo dime el nombre, no des más información.\")\n",
        "\n",
        "# Se genera un prompt utilizando la plantilla\n",
        "prompt = prompt_template.invoke({\"genre\": \"fantasía\"})\n",
        "prompt"
      ],
      "metadata": {
        "id": "-0n9h3P53O0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "response.content"
      ],
      "metadata": {
        "id": "36gmDBuA4i_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se pueden generar plantillas para los prompts y las instrucciones a nivel system:"
      ],
      "metadata": {
        "id": "qSpx-WE_5qKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea una plantilla de prompts.\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"Eres un asistente experto en libros del género {genre}. Responde a la pregunta solo diciendo el nombre del libro que se te pide, no des más información.\"),\n",
        "    (\"user\", \"Dime el nombre de un libro famoso de {genre}.\")\n",
        "])\n",
        "\n",
        "prompt = prompt_template.invoke({\"genre\": \"ciencia ficción\"})\n",
        "prompt"
      ],
      "metadata": {
        "id": "gxWE_J8M5uqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "response.content"
      ],
      "metadata": {
        "id": "JtD_lLlT7JcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea una plantilla de prompts.\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    SystemMessage(\"Eres un asistente experto en libros del género {genre}. Responde a la pregunta solo diciendo el nombre del libro que se te pide, no des más información.\"),\n",
        "    HumanMessage(\"Dime el nombre de un libro famoso de {genre}.\"),\n",
        "])\n",
        "\n",
        "prompt = prompt_template.invoke({\"genre\": \"ciencia ficción\"})\n",
        "prompt"
      ],
      "metadata": {
        "id": "xqbN_VqHW3bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "response.content"
      ],
      "metadata": {
        "id": "gKC7IPHPW-uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Más información en: https://python.langchain.com/docs/concepts/prompt_templates/"
      ],
      "metadata": {
        "id": "pXTacJ7kXCbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "O6kHzeGo7YTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few shot prompting y One-Shot prompting"
      ],
      "metadata": {
        "id": "VntslaDG7aJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El Few-Shot Prompting es una técnica en la que se proporcionan al modelo de lenguaje algunos ejemplos dentro del prompt para ayudarlo a entender el patrón de la tarea antes de generar una respuesta. El One Shot prompting consiste en pasar un solo ejemplo."
      ],
      "metadata": {
        "id": "47dwK4YS7yqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existen plantillas de LangChain que ayudan a la organización de este tipo de prompts."
      ],
      "metadata": {
        "id": "Co-7g0Sa-4K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de ejemplos de few-shot\n",
        "examples = [\n",
        "    {\"word\": \"happy\", \"answer\": \"Dialektico está triste\"},\n",
        "    {\"word\": \"big\", \"answer\": \"Dialektico es pequeño\"},\n",
        "    {\"word\": \"weak\", \"answer\": \"Dialektico es fuerte\"},\n",
        "]\n",
        "\n",
        "examples"
      ],
      "metadata": {
        "id": "b6JKJR_b_bKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate.from_template(\"Palabra Clave: {word}\\nRespuesta esperada: {answer}\")\n",
        "example_prompt"
      ],
      "metadata": {
        "id": "gdhWg9A0_etL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_prompt.invoke(examples[0]).to_string())"
      ],
      "metadata": {
        "id": "1DvB29H9B8iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación del FewShotPromptTemplate\n",
        "prompt  = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"\"\"Ejemplos:\"\"\",\n",
        "    suffix=\"De acuerdo a los ejemplos, responde a la siguiente Palabra Clave: {input}\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\"\n",
        ")\n",
        "\n",
        "prompt = prompt.invoke({\"input\": \"Flaco\"}).to_string()\n",
        "prompt"
      ],
      "metadata": {
        "id": "g-C2C3M17Zgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "response.content"
      ],
      "metadata": {
        "id": "l9jU1jkA_z0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "lmzAo25WeGTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streams en respuestas"
      ],
      "metadata": {
        "id": "OLqkrzp8eGCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los LLM pueden tardar varios segundos en generar una respuesta completa a una consulta. Esto es mucho más lento que el umbral de ~200-300 ms en el que una aplicación se siente receptiva para el usuario final.\n",
        "\n",
        "La estrategia clave para hacer que la aplicación se sienta más receptiva es mostrar un progreso intermedio, es decir, transmitir la salida del modelo token por token.\n",
        "\n",
        "LangChain permite esto mediante un \"stream\" de las respuestas:"
      ],
      "metadata": {
        "id": "kqOXIL-eelox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "for chunk in model.stream(\"¿Cuál es el nombre completo de Da Vinci?\"):\n",
        "    chunks.append(chunk)\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "-muOeu7Cdhhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Más información: https://python.langchain.com/docs/how_to/streaming/"
      ],
      "metadata": {
        "id": "acIcJjxge0-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "XaI3o2U8HgyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dialektico Machine learning practices © 2025 by Daniel Antonio García Escobar\n",
        "# is licensed under CC BY-NC 4.0. To view a copy of this license,\n",
        "# visit https://creativecommons.org/licenses/by-nc/4.0/\n",
        "\n",
        "# Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\n",
        "# Public License"
      ],
      "metadata": {
        "id": "7ly3JFP2WEOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}